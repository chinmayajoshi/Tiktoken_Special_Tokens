{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Count: 100277\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer (you can replace 'gpt-3.5-turbo' with your desired model)\n",
    "tokenizer = tiktoken.encoding_for_model('gpt-4-')\n",
    "print(f\"Token Count: {tokenizer.n_vocab}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID: 100257, Token: <|endoftext|>\n",
      "Token ID: 100258, Token: <|fim_prefix|>\n",
      "Token ID: 100259, Token: <|fim_middle|>\n",
      "Token ID: 100260, Token: <|fim_suffix|>\n",
      "Token ID: 100276, Token: <|endofprompt|>\n"
     ]
    }
   ],
   "source": [
    "for i in range(tokenizer.n_vocab):\n",
    "    try:\n",
    "        # Get the bytes for this token\n",
    "        token_bytes = tokenizer.decode_single_token_bytes(i)\n",
    "        # Convert to string\n",
    "        token_str = token_bytes.decode('utf-8')\n",
    "        # Check if it matches our pattern\n",
    "        if token_str.startswith('<|') and token_str.endswith('|>'):\n",
    "            print(f\"Token ID: {i}, Token: {token_str}\")\n",
    "    except Exception:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID: 20112, Token: <?>\n",
      "Token ID: 21806, Token: <>\n",
      "Token ID: 72798, Token: <()>\n",
      "Token ID: 75194, Token: <?>>\n",
      "Token ID: 100257, Token: <|endoftext|>\n",
      "Token ID: 100258, Token: <|fim_prefix|>\n",
      "Token ID: 100259, Token: <|fim_middle|>\n",
      "Token ID: 100260, Token: <|fim_suffix|>\n",
      "Token ID: 100276, Token: <|endofprompt|>\n"
     ]
    }
   ],
   "source": [
    "for i in range(tokenizer.n_vocab):\n",
    "    try:\n",
    "        # Get the bytes for this token\n",
    "        token_bytes = tokenizer.decode_single_token_bytes(i)\n",
    "        # Convert to string\n",
    "        token_str = token_bytes.decode('utf-8')\n",
    "        # Check if it matches our pattern\n",
    "        if token_str.startswith('<') and token_str.endswith('>'):\n",
    "            print(f\"Token ID: {i}, Token: {token_str}\")\n",
    "    except Exception:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID: 13, Token: .\n",
      "Token ID: 497, Token: ..\n",
      "Token ID: 1131, Token: ...\n",
      "Token ID: 1975, Token: ....\n",
      "Token ID: 4095, Token: ........\n",
      "Token ID: 8054, Token: ................\n",
      "Token ID: 16971, Token: ................................\n",
      "Token ID: 18575, Token: .....\n",
      "Token ID: 29249, Token: ......\n",
      "Token ID: 36434, Token: .).\n",
      "Token ID: 37049, Token: .'.\n",
      "Token ID: 38011, Token: .\".\n",
      "Token ID: 43369, Token: ................................................................\n",
      "Token ID: 49711, Token: .......\n",
      "Token ID: 57341, Token: ........................\n",
      "Token ID: 62073, Token: .........\n",
      "Token ID: 86487, Token: .$.\n",
      "Token ID: 99501, Token: .\").\n"
     ]
    }
   ],
   "source": [
    "for i in range(tokenizer.n_vocab):\n",
    "    try:\n",
    "        # Get the bytes for this token\n",
    "        token_bytes = tokenizer.decode_single_token_bytes(i)\n",
    "        # Convert to string\n",
    "        token_str = token_bytes.decode('utf-8')\n",
    "        # Check if it matches our pattern\n",
    "        if token_str.startswith('.') and token_str.endswith('.'):\n",
    "            print(f\"Token ID: {i}, Token: {token_str}\")\n",
    "    except Exception:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID: 2, Token: #\n",
      "Token ID: 567, Token: ##\n",
      "Token ID: 827, Token: ####\n",
      "Token ID: 1381, Token: ########\n",
      "Token ID: 2488, Token: ################\n",
      "Token ID: 5134, Token: ################################\n",
      "Token ID: 13367, Token: ################################################################\n",
      "Token ID: 14711, Token: ###\n",
      "Token ID: 17913, Token: ############\n",
      "Token ID: 30954, Token: ################################################\n",
      "Token ID: 33141, Token: ############################################################################\n",
      "Token ID: 41586, Token: ################################################################################\n",
      "Token ID: 68431, Token: #####\n",
      "Token ID: 70161, Token: ########################################################################\n",
      "Token ID: 78229, Token: ######\n",
      "Token ID: 83678, Token: ########################################\n",
      "Token ID: 85399, Token: ########################\n",
      "Token ID: 89200, Token: ############################\n",
      "Token ID: 91138, Token: ########################################################\n",
      "Token ID: 98944, Token: ############################################################\n",
      "Token ID: 98964, Token: #######\n"
     ]
    }
   ],
   "source": [
    "for i in range(tokenizer.n_vocab):\n",
    "    try:\n",
    "        # Get the bytes for this token\n",
    "        token_bytes = tokenizer.decode_single_token_bytes(i)\n",
    "        # Convert to string\n",
    "        token_str = token_bytes.decode('utf-8')\n",
    "        # Check if it matches our pattern\n",
    "        if token_str.startswith('#') and token_str.endswith('#'):\n",
    "            print(f\"Token ID: {i}, Token: {token_str}\")\n",
    "    except Exception:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID: 62, Token: _\n",
      "Token ID: 565, Token: __\n",
      "Token ID: 2179, Token: ____\n",
      "Token ID: 4067, Token: ________\n",
      "Token ID: 6101, Token: ___\n",
      "Token ID: 7841, Token: ________________\n",
      "Token ID: 17924, Token: ________________________________\n",
      "Token ID: 42483, Token: ____________\n",
      "Token ID: 48032, Token: ________________________________________________________________\n",
      "Token ID: 50480, Token: __.__\n",
      "Token ID: 63850, Token: _$_\n",
      "Token ID: 81617, Token: _____\n",
      "Token ID: 84030, Token: __,__\n",
      "Token ID: 84200, Token: _-_\n"
     ]
    }
   ],
   "source": [
    "for i in range(tokenizer.n_vocab):\n",
    "    try:\n",
    "        # Get the bytes for this token\n",
    "        token_bytes = tokenizer.decode_single_token_bytes(i)\n",
    "        # Convert to string\n",
    "        token_str = token_bytes.decode('utf-8')\n",
    "        # Check if it matches our pattern\n",
    "        if token_str.startswith('_') and token_str.endswith('_'):\n",
    "            print(f\"Token ID: {i}, Token: {token_str}\")\n",
    "    except Exception:\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
